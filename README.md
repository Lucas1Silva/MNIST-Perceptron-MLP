# ğŸ“Œ ClassificaÃ§Ã£o de DÃ­gitos MNIST com Perceptron e MLP

Este projeto implementa dois modelos de classificaÃ§Ã£o para o dataset **MNIST** (conjunto de dÃ­gitos escritos Ã  mÃ£o), utilizando:
- **Perceptron Simples:** Modelo de aprendizado supervisionado baseado em um Ãºnico neurÃ´nio.
- **MLP (Multi-Layer Perceptron):** Rede neural densa com mÃºltiplas camadas para melhorar a precisÃ£o.

## ğŸ“– DescriÃ§Ã£o do Problema
O objetivo Ã© classificar corretamente imagens de dÃ­gitos de **0 a 9** a partir do dataset **MNIST**, amplamente utilizado para aprendizado de **Machine Learning** e **Deep Learning**.

O dataset contÃ©m:
- **60.000 imagens de treinamento** (28x28 pixels, tons de cinza)
- **10.000 imagens de teste**

Cada imagem Ã© representada por **784 pixels achatados** (28x28 â†’ vetor de 784 dimensÃµes).

---

## ğŸ“Œ Estrutura do CÃ³digo

O cÃ³digo Ã© dividido em cÃ©lulas para melhor organizaÃ§Ã£o e eficiÃªncia no **Kaggle Notebook**. Cada etapa do pipeline pode ser executada separadamente.

### ğŸ“Œ 1ï¸âƒ£ ImportaÃ§Ã£o de Bibliotecas
Importamos **NumPy, TensorFlow, Matplotlib, Seaborn** e outras bibliotecas para manipulaÃ§Ã£o de dados e modelagem.

### ğŸ“Œ 2ï¸âƒ£ Carregamento e PrÃ©-processamento dos Dados
- O dataset MNIST Ã© carregado diretamente dos arquivos **IDX** no Kaggle.
- Os dados sÃ£o normalizados (**MinMaxScaler**) para ficar no intervalo `[0,1]`.
- O conjunto de dados Ã© dividido em **Treino (80%)** e **ValidaÃ§Ã£o/Teste (20%)**.

### ğŸ“Œ 3ï¸âƒ£ ImplementaÃ§Ã£o do Perceptron
Criamos uma classe **Perceptron** do zero, que:
- Inicializa pesos e viÃ©s (`w, b`).
- Utiliza a funÃ§Ã£o de ativaÃ§Ã£o **step function**.
- Atualiza os pesos via **Regra de Aprendizado do Perceptron**.
- Treina por `n_epochs` e armazena os erros ao longo do tempo.

### ğŸ“Œ 4ï¸âƒ£ ImplementaÃ§Ã£o do MLP
Criamos uma **Rede Neural Densa (MLP)** usando `TensorFlow` com:
- **128 neurÃ´nios na camada oculta** (ReLU).
- **10 neurÃ´nios na camada de saÃ­da** (Softmax).
- **Adam Optimizer e Sparse Categorical Crossentropy**.

### ğŸ“Œ 5ï¸âƒ£ Treinamento e AvaliaÃ§Ã£o dos Modelos
- O **Perceptron** Ã© treinado e avaliado com `np.mean(y_pred == y_test)`.
- O **MLP** Ã© treinado com `model.fit()` e avaliado com `model.evaluate()`.

### ğŸ“Œ 6ï¸âƒ£ VisualizaÃ§Ã£o dos Resultados
âœ… **Amostras do Dataset**  
âœ… **GrÃ¡fico de ConvergÃªncia do Perceptron**  
âœ… **HistÃ³rico de Treinamento do MLP (Perda e AcurÃ¡cia)**  
âœ… **Matriz de ConfusÃ£o** para anÃ¡lise de erros.

---

## ğŸ“Š **Resultados Obtidos**
| Modelo       | AcurÃ¡cia no Teste |
|-------------|----------------|
| **Perceptron** | `98.98%` |
| **MLP**        | `A ser obtida apÃ³s treinamento` |

âš  **Nota:** A acurÃ¡cia pode variar a cada execuÃ§Ã£o devido Ã  inicializaÃ§Ã£o dos pesos.

---

## ğŸš€ ConclusÃ£o
Este projeto demonstrou a diferenÃ§a de desempenho entre um **Perceptron Simples** e uma **Rede Neural Profunda (MLP)** para a classificaÃ§Ã£o de dÃ­gitos.  
- O **Perceptron** funciona bem para problemas lineares, mas nÃ£o consegue generalizar padrÃµes complexos.
- O **MLP**, por ter mÃºltiplas camadas, oferece maior poder de representaÃ§Ã£o e melhora o desempenho.

ğŸ“Œ **PrÃ³ximos Passos:**  
- Expandir a rede neural adicionando **mais camadas** (Deep Learning).  
- Experimentar **CNNs (Redes Neurais Convolucionais)** para melhorar a acurÃ¡cia.

---

## ğŸ“¢ Conecte-se Comigo!  
Caso tenha dÃºvidas, sugestÃµes ou queira trocar experiÃªncias sobre **Machine Learning e Deep Learning**, entre em contato comigo no LinkedIn:  

ğŸ”— **[Lucas Silva](https://www.linkedin.com/in/Lucas19Silva/)**  

ğŸš€ Vamos crescer juntos na jornada de IA!  
